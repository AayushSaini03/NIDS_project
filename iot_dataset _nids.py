# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fthLTTr7WLAImxPVBxGoHWsItepAg7AV
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("azalhowaide/iot-dataset-for-intrusion-detection-systems-ids")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, precision_recall_curve

botnet_df_v2 = pd.read_csv('/root/.cache/kagglehub/datasets/azalhowaide/iot-dataset-for-intrusion-detection-systems-ids/versions/2/BoTNeTIoT-L01-v2.csv')
botnet_df_no_duplicates = pd.read_csv('/root/.cache/kagglehub/datasets/azalhowaide/iot-dataset-for-intrusion-detection-systems-ids/versions/2/BotNeTIoT-L01_label_NoDuplicates.csv')

sample_botnet_df_v2 = botnet_df_v2.sample(n=1000, random_state=1)
sample_botnet_df_no_duplicates = botnet_df_no_duplicates.sample(n=1000, random_state=1)

# Save the sampled data
sample_botnet_df_v2.to_csv('sample_botnet_df_v2.csv', index=False)
sample_botnet_df_no_duplicates.to_csv('sample_botnet_df_no_duplicates.csv', index=False)

# botnet_df_v2 features
print('botnet_df_v2 features:')
print(sample_botnet_df_v2.columns)
print('\n')
print('*'*50)

print('-'*10, 'HEAD', '-'*10)
print(botnet_df_v2.head())
print('\n')

print('-'*10, 'DESCRIBE', '-'*10)
print(botnet_df_v2.describe())
print('\n')

print('-'*10, 'INFO', '-'*10)
print(botnet_df_v2.info())
print('\n')

print('-'*10, 'MISSING VALUES', '-'*10)
print(botnet_df_v2.isnull().sum())
print('\n')

print('-'*10, 'DATA TYPES', '-'*10)
print(botnet_df_v2.dtypes)
print('\n')

# Check for unique values
print('-'*10, 'UNIQUE VALUES', '-'*10)
print(botnet_df_v2.nunique())
print('\n')

print('-'*10, 'UNIQUE VALUES FOR ATTACK AND ATTACK_SUBTYPE', '-'*10)
print(botnet_df_v2['Attack'].unique())
print(botnet_df_v2['Attack_subType'].unique())
print('\n')

# Pie chart for attack and device_name
print('-'*10, 'PIE CHART FOR ATTACK AND DEVICE_NAME', '-'*10)
plt.figure(figsize=(10, 10))
plt.subplot(2, 1, 1)
botnet_df_v2['Attack'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Attack')
plt.subplot(2, 1, 2)
botnet_df_v2['Device_Name'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Device_Name')
plt.show()

df = pd.read_csv('sample_botnet_df_v2.csv')
print(df.columns)

le = preprocessing.LabelEncoder()
df['Device_Name'] = le.fit_transform(df['Device_Name'])
df['Attack'] = le.fit_transform(df['Attack'])
df['Attack_subType'] = le.fit_transform(df['Attack_subType'])

print('-'*10, 'DATA TYPES', '-'*10)
print(df.dtypes)
print('\n')

print('Device_Name:', df['Device_Name'].unique())
print('Attack:', df['Attack'].unique())
print('Attack_subType:', df['Attack_subType'].unique())

sns.countplot(x='Attack', hue='label', data=df)
plt.title('Attack vs Label')
plt.show()

# Plot 'Attack_subType' vs 'label'
sns.countplot(x='Attack_subType', hue='label', data=df)
plt.title('Attack_subType vs Label')
plt.show()

# Drop attack and attack subtype
df = df.drop(['Attack', 'Attack_subType'], axis=1)

print('-'*10, 'Data Types', '-'*10)
print(df.dtypes)

# Save the data
df.to_csv('BoTNeTIoT-L01-v2-prepared.csv', index=False)

df = pd.read_csv('BoTNeTIoT-L01-v2-prepared.csv')

def drop_highly_correlated_features(df, threshold=0.95):
    df_copy = df.copy()
    corr_matrix = df_copy.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    df_copy.drop(columns=to_drop, inplace=True)
    return df_copy, corr_matrix, to_drop

df, corr, dropped_features = drop_highly_correlated_features(df)

print("Dropped features:", dropped_features)
print("Remaining features:", df.columns.tolist())
corr_after = df.corr().abs()
plt.figure(figsize=(30, 30))
sns.heatmap(corr_after, annot=True, cmap="coolwarm")
plt.title('Correlation Heatmap (After Feature Removal)')
plt.show()

# print the correlation matrix
print(corr_after)

features = df.drop(columns=['label'])
target = df['label']

print('-'*10, 'FEATURES AND TARGET', '-'*10)
print('Features:', features.columns.tolist())
print('Target:', target.name)

# Data Splitting
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0, stratify=target)

# Save the split data
print('-' * 10, 'DATA SPLITS', '-' * 10)
print('Training Features Shape:', X_train.shape)
print('Test Features Shape:', X_test.shape)
print('Training Target Shape:', y_train.shape)
print('Test Target Shape:', y_test.shape)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save the scaled data
pd.DataFrame(X_train_scaled, columns=X_train.columns).to_csv('X_train_scaled.csv', index=False)
pd.DataFrame(X_test_scaled, columns=X_test.columns).to_csv('X_test_scaled.csv', index=False)

X_train.to_csv('X_train.csv', index=False)
X_test.to_csv('X_test.csv', index=False)
y_train.to_csv('y_train.csv', index=False)
y_test.to_csv('y_test.csv', index=False)

# Read data
X_train_scaled = pd.read_csv('X_train_scaled.csv')
X_train = pd.read_csv('X_train.csv')
y_train = pd.read_csv('y_train.csv')

X_test_scaled = pd.read_csv('X_test_scaled.csv')
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')
print('Data loaded')
print('X_train_scaled shape:', X_train_scaled.shape)
print('X_train shape:', X_train.shape)
print('y_train shape:', y_train.shape)
print('X_test_scaled shape:', X_test_scaled.shape)
print('X_test shape:', X_test.shape)
print('y_test shape:', y_test.shape)

y_train = y_train.values.ravel()

"""Random Forest"""

def train_random_forest(X_train, y_train, n_estimators=100, top_n_features=10):
    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, verbose=1, n_jobs=-1)
    rf.fit(X_train, y_train)

    feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)
    important_features = feature_importance.nlargest(top_n_features).index

    feature_importance.nlargest(top_n_features).plot(kind='barh', title='Feature Importance')
    plt.show()

    return rf, important_features

def perform_grid_search(X_train, y_train, param_grid):
    rf = RandomForestClassifier(random_state=42, n_jobs=-1)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)

    print("Best Parameters:", grid_search.best_params_)
    print("Best Score:", grid_search.best_score_)
    return grid_search.best_estimator_

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
}

rf, important_features = train_random_forest(X_train_scaled, y_train, top_n_features=10)

top_X_train = X_train_scaled[important_features]
top_X_test = X_test_scaled[important_features]

best_model = perform_grid_search(top_X_train, y_train, param_grid)

y_pred = best_model.predict(top_X_test)
y_pred_proba = best_model.predict_proba(top_X_test)[:, 1]
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Load the model
filename = 'best_model.sav'
pickle.dump(best_model, open(filename, 'wb'))

# Load the model
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(top_X_test, y_test)
print(result)

print('*'*10 + 'Results' + '*'*10)
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1 Score:', f1_score(y_test, y_pred))
print('ROC AUC Score:', roc_auc_score(y_test, y_pred_proba))

"""NN"""

mlp = MLPClassifier(max_iter=1000)
mlp.fit(top_X_train, y_train)

# Cross-validation score
print('*' * 10 + ' Cross Validation Score ' + '*' * 10)
print(cross_val_score(mlp, top_X_train, y_train, cv=5, scoring='accuracy').mean())

# Define Hyperparameter Grid for MLP
param_grid_mlp = {
    'hidden_layer_sizes': [(100,), (200,), (300,)],
    'activation': ['logistic', 'tanh', 'relu'],
}

# Results
print('*' * 10 + ' Results ' + '*' * 10)
print('Accuracy:', round(accuracy_score(y_test, mlp.predict(top_X_test)), 3))
print('Precision:', round(precision_score(y_test, mlp.predict(top_X_test)), 3))
print('Recall:', round(recall_score(y_test, mlp.predict(top_X_test)), 3))
print('F1 Score:', round(f1_score(y_test, mlp.predict(top_X_test)), 3))
print('ROC AUC Score:', round(roc_auc_score(y_test, mlp.predict_proba(top_X_test)[:, 1]), 3))
# Load the model
# Save the trained MLP model
filename = 'mlp_model.sav'
pickle.dump(mlp, open(filename, 'wb'))

loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(top_X_test, y_test)
print(result)

"""CNN"""

def build_cnn(input_shape):
    model = Sequential([
        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),
        MaxPooling1D(pool_size=2),
        Conv1D(filters=64, kernel_size=3, activation='relu'),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Reshape data for CNN (assuming time series-like data with 1D structure)
top_X_train_cnn = top_X_train.values.reshape((top_X_train.shape[0], top_X_train.shape[1], 1))
top_X_test_cnn = top_X_test.values.reshape((top_X_test.shape[0], top_X_test.shape[1], 1))

# Initialize and train CNN model
cnn_model = build_cnn(input_shape=(top_X_train.shape[1], 1))
history = cnn_model.fit(top_X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2)
# Save CNN model
cnn_model.save('cnn_model.h5')

# Results
cnn_predictions = (cnn_model.predict(top_X_test_cnn) > 0.5).astype("int32")
print('Results')
print('Accuracy:', accuracy_score(y_test, cnn_predictions))
print('Precision:', precision_score(y_test, cnn_predictions))
print('Recall:', recall_score(y_test, cnn_predictions))
print('F1 Score:', f1_score(y_test, cnn_predictions))
print('ROC AUC Score:', roc_auc_score(y_test, cnn_model.predict(top_X_test_cnn)))

# Plot Results
fpr, tpr, thresholds = roc_curve(y_test, cnn_model.predict(top_X_test_cnn))
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

precision, recall, thresholds = precision_recall_curve(y_test, cnn_model.predict(top_X_test_cnn))

plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.show()
# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, cnn_predictions)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plot loss and accuracy
plt.figure()
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['loss'], label='loss')
plt.xlabel('Epoch')
plt.ylabel('Loss/Accuracy')
plt.title('Loss and Accuracy')
plt.legend()
plt.show()